<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>net API documentation</title>
<meta name="description" content="Multifidelity Surrogate Modeling via Directed Networks â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>net</code></h1>
</header>
<section id="section-intro">
<p>Multifidelity Surrogate Modeling via Directed Networks </p>
<p>Author: Alex Gorodetsky, goroda@umich.edu</p>
<p>Copyright (c) 2020, Alex Gorodetsky</p>
<p>License: MIT</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;  Multifidelity Surrogate Modeling via Directed Networks 

Author: Alex Gorodetsky, goroda@umich.edu

Copyright (c) 2020, Alex Gorodetsky

License: MIT
&#34;&#34;&#34;

import copy
try:
    import queue.SimpleQueue as SimpleQueue
except ImportError:
    from queue import Queue as SimpleQueue
from functools import partial

import numpy as np

import networkx as nx
# print(&#34;NetworkX Version = &#34;, nx.__version__)

import scipy.optimize as sciopt

from functools import partial

pyapprox_is_installed = True
try:
    from pyapprox.l1_minimization import nonlinear_basis_pursuit, lasso
except ModuleNotFoundError:
    pyapprox_is_installed = False


def least_squares(target, predicted, std=1e0):
    &#34;&#34;&#34; Evaluate the least squares objective function 

    Parameters
    ----------
    target : np.ndarray (nobs)
        The observations

    predicted : np.ndarray (nobs)
        The model predictions of the observations

    std : float
        The standard deviation of the I.I.D noise

    Returns
    -------
    obj : float
        The value of the least squares objective function
    
    grad : np.ndarray (nobs)
        The gradient of ``obj``
    &#34;&#34;&#34;
    resid = target - predicted
    obj = np.dot(resid, resid) * 0.5 * std**-2
    grad = - std**-2 * resid
    return obj, grad

def lin(param, xinput):
    &#34;&#34;&#34;A linear parametric model 

    Parameters
    ----------
    param : np.ndarray (nparams)
       The parameters of the model

    xinput : np.ndarray (nsamples,nparams)
       The independent variables of the model

    Returns
    -------
    vals : np.ndarray (nsamples)
      Evaluation of the linear model

    grad : np.ndarray (nsamples,nparams)
      gradient of the linear model with respect to the model parameters
    &#34;&#34;&#34;
    one = np.ones((xinput.shape[0], 1))
    grad = np.concatenate((one, xinput), axis=1)
    return param[0] + np.dot(param[1:], xinput.T), grad

def monomial_1d_lin(param, xinput):
    &#34;&#34;&#34;Linear Model with Monomial basis functions

    p[0]+sum(x**p[1:])

    Parameters
    ----------
    param : np.ndarray (nparams)
       The parameters of the model

    xinput : np.ndarray (nsamples,nparams)
       The independent variables of the model

    Returns
    -------
    vals : np.ndarray (nsamples)
        Evaluation of the linear model

    grad : np.ndarray (nsamples,nparams)
      gradient of the linear model with respect to the model parameters
    &#34;&#34;&#34;
    basis = xinput**np.arange(param.shape[0])[np.newaxis, :]
    vals = basis.dot(param)
    grad = basis
    return vals, grad

class MFSurrogate():
    &#34;&#34;&#34;Multifidelity surrogate

    A surrogate consists of a graph where the edges and nodes are functions
    Each node represents a particular information sources and the edges
    describe the relationships between the information sources

    &#34;&#34;&#34;
    def __init__(self, graph, roots, copy_data=True):
        &#34;&#34;&#34;Initialize a multifidelity surrogate by providing a graph and the
        roots of the graph

        Parameters
        ----------
        graph : networkx.graph
            The graphical representation of the MF network

        roots : list
            The ids of every root node in the network

        copy_data : boolean
           True - perform a deep copy of graph and roots
           False - just use a shallow copy (this is dangerous as many functions
           change the internal shape)
        &#34;&#34;&#34;
        if copy_data:
            self.roots = copy.deepcopy(roots)
            self.graph = copy.deepcopy(graph)
        else:
            self.roots = roots
            self.graph = graph
            print(&#39;warning MFSurrogate not copying data. proceed with caution&#39;)
        self.nparam = graph_to_vec(self.graph).shape[0]

    def get_nparam(self):
        &#34;&#34;&#34;Number of parameters parameterizing the graph

        Returns
        -------
        nparam : integer
            The number of all the unknown parameters in the MF surrogate&#34;&#34;&#34;
        return self.nparam

    def forward(self, xinput, target_node):
        &#34;&#34;&#34;Evaluate the surrogate output at target_node by considering the
        subgraph of all ancestors of this node

        Parameters
        ----------
        xinput : np.ndarray (nsamples,nparams)
            The independent variables of the model

        target_node : integer
            The id of the node under consideration

        Returns
        -------
        This function adds the following attributes to the underlying graph

        eval :
            stores the evaluation of the function represented by the
            particular node / edge the evaluations at the nodes are
            cumulative (summing up all ancestors) whereas the edges are local

        pre-grad : np.ndarray
            internal attribute needed for accounting

        parents-left : set
            internal attribute needed for accounting

        &#34;&#34;&#34;
        anc = nx.ancestors(self.graph, target_node)
        anc_and_target = anc.union(set([target_node]))
        relevant_root_nodes = anc.intersection(self.roots)

        # Evaluate the target nodes and all ancestral nodes and put the root
        # nodes in a queue
        queue = SimpleQueue()
        for node in anc_and_target:
            pval, pgrad = self.graph.nodes[node][&#39;func&#39;](
                self.graph.nodes[node][&#39;param&#39;], xinput)
            self.graph.nodes[node][&#39;eval&#39;] = pval
            self.graph.nodes[node][&#39;pre_grad&#39;] = pgrad
            self.graph.nodes[node][&#39;parents_left&#39;] = set(
                self.graph.predecessors(node))

            if node in relevant_root_nodes:
                queue.put(node)

        while not queue.empty():

            node = queue.get()
            feval = self.graph.nodes[node][&#39;eval&#39;]
            for child in self.graph.successors(node):
                if child in anc_and_target:
                    pval, pgrad = self.graph.edges[node, child][&#39;func&#39;](
                        self.graph.edges[node, child][&#39;param&#39;], xinput)

                    self.graph.nodes[child][&#39;eval&#39;] += feval * pval

                    ftile = np.tile(feval.reshape(
                        (feval.shape[0], 1)), (1, pgrad.shape[1]))
                    self.graph.edges[node, child][&#39;pre_grad&#39;] = ftile * pgrad
                    self.graph.edges[node, child][&#39;eval&#39;] = pval

                    self.graph.nodes[child][&#39;parents_left&#39;].remove(node)

                    if self.graph.nodes[child][&#39;parents_left&#39;] == set():
                        queue.put(child)

        return self.graph.nodes[node][&#39;eval&#39;], anc

    def backward(self, target_node, deriv_pass, ancestors=None):
        &#34;&#34;&#34;Perform a backward computation to compute the derivatives for all
        parameters that affect the target node

        Parameters
        ----------
        target_node : integer
            The id of the node under consideration

        deriv_pass : np.ndarray (nparams)
            A gradient vector

        Returns
        -------
        derivative : np.ndarray(nparams)
            A vector containing the derivative of all parameters
        &#34;&#34;&#34;

        if ancestors is None:
            ancestors = nx.ancestors(self.graph, target_node)

        anc_and_target = ancestors.union(set([target_node]))


        # Evaluate the node
        self.graph.nodes[target_node][&#39;pass_down&#39;] = deriv_pass

        # Gradient with respect to beta
        self.graph.nodes[target_node][&#39;derivative&#39;] = \
            np.dot(self.graph.nodes[target_node][&#39;pass_down&#39;],
                   self.graph.nodes[target_node][&#39;pre_grad&#39;])

        queue = SimpleQueue()
        queue.put(target_node)

        for node in ancestors:
            self.graph.nodes[node][&#39;children_left&#39;] = set(
                self.graph.successors(node)).intersection(anc_and_target)
            self.graph.nodes[node][&#39;pass_down&#39;] = 0.0
            self.graph.nodes[node][&#39;derivative&#39;] = 0.0


        while not queue.empty():
            node = queue.get()

            pass_down = self.graph.nodes[node][&#39;pass_down&#39;]
            for parent in self.graph.predecessors(node):
                self.graph.nodes[parent][&#39;pass_down&#39;] += \
                    pass_down * self.graph.edges[parent, node][&#39;eval&#39;]
                self.graph.edges[parent, node][&#39;derivative&#39;] = \
                    np.dot(pass_down,self.graph.edges[parent, node][&#39;pre_grad&#39;])
                self.graph.nodes[parent][&#39;derivative&#39;] += \
                    np.dot(pass_down * self.graph.edges[parent, node][&#39;eval&#39;],
                           self.graph.nodes[parent][&#39;pre_grad&#39;])

                self.graph.nodes[parent][&#39;children_left&#39;].remove(node)
                if self.graph.nodes[parent][&#39;children_left&#39;] == set():
                    queue.put(parent)

        return self.get_derivative()

    def set_param(self, param):
        &#34;&#34;&#34;Set the parameters for the graph

        Parameters
        ----------
        param : np.ndarray (nparams)
            A flattened array containing all parameters of the MF surrogate
        &#34;&#34;&#34;
        self.graph = vec_to_graph(param, self.graph, attribute=&#39;param&#39;)

    def get_param(self):
        &#34;&#34;&#34;Get the parameters of the graph &#34;&#34;&#34;
        return graph_to_vec(self.graph, attribute=&#39;param&#39;)

    def get_derivative(self):
        &#34;&#34;&#34;Get a vector of derivatives of each parameter &#34;&#34;&#34;
        return graph_to_vec(self.graph, attribute=&#39;derivative&#39;)

    def get_evals(self):
        &#34;&#34;&#34;Get the evaluations at each node &#34;&#34;&#34;
        return [self.graph.nodes[node][&#39;eval&#39;] for node in self.graph.nodes]

    def zero_derivatives(self):
        &#34;&#34;&#34;Set all the derivative attributes to zero

        Used prior to computing a new derivative to clear out previous sweep
        &#34;&#34;&#34;
        self.graph = vec_to_graph(
            np.zeros(self.nparam), self.graph, attribute=&#39;derivative&#39;)

    def zero_attributes(self):
        &#34;&#34;&#34;Zero all attributes except &#39;func&#39; and &#39;param&#39; &#34;&#34;&#34;

        atts = [&#39;eval&#39;, &#39;pass_down&#39;, &#39;pre_grad&#39;, &#39;derivative&#39;,
                &#39;children_left&#39;, &#39;parents_left&#39;]
        for att in atts:
            for node in self.graph.nodes:
                try:
                    self.graph.nodes[node][att] = 0.0
                except: # what exception is it?
                    continue
            for edge in self.graph.edges:
                try:
                    self.graph.edges[edge][att] = 0.0
                except: # what exception is it?
                    continue


    def train(self, param0in, nodes, xtrain, ytrain, stdtrain, niters=200,
              func=least_squares,
              verbose=False, warmup=True, opts=dict()):
        &#34;&#34;&#34;Train the multifidelity surrogate.

        This is the main entrance point for data-driven training.

        Parameters
        ----------
        param0in : np.ndarray (nparams)
            The initial guess for the parameters

        nodes : list
            A list of nodes for which data is available

        xtrain : list
            A list of input features for each node in *nodes*

        ytrain : list
            A list of output values for each node in *nodes*

        stdtrain : float
            The standard devaition for data for each node in *nodes*

        niters : integer
            The number of optimization iterations

        func : callable
            A scalar valued objective function with the signature

            ``func(target, predicted) -&gt;  val (float), grad (np.ndarray)``

            where ``target`` is a np.ndarray of shape (nobs)
            containing the observations and ``predicted`` is a np.ndarray of
            shape (nobs) containing the model predictions of the observations

        verbose : integer
            The verbosity level

        warmup : boolean
            Specify whether or not to progressively find a good guess before
            optimizing

        opts : dictionary
            Specify the type of loss function: &#39;lstsq&#39; for squared error, anything else for L1 regularization, &#39;lambda&#39; for regularization value
        
        Returns
        -------
        Upon completion of this function, the parameters of the graph are set
        to the values that best fit the data, as defined by *func*
        &#34;&#34;&#34;
        bounds = list(zip([-np.inf]*self.nparam, [np.inf]*self.nparam))
        param0 = copy.deepcopy(param0in)

        # options = {&#39;maxiter&#39;:20, &#39;disp&#39;:False, &#39;gtol&#39;:1e-10, &#39;ftol&#39;:1e-18}
        options = {&#39;maxiter&#39;:20, &#39;disp&#39;:False, &#39;gtol&#39;:1e-10, &#39;ftol&#39;:1e-18}

        # Warming up
        if warmup is True:
            for node in nodes:

                node_list = nodes[node-1:node]
                x_list = xtrain[node-1:node]
                y_list = ytrain[node-1:node]
                std_list = stdtrain[node-1:node]

                res = sciopt.minimize(
                    optimize_obj, param0,
                    args=(func, self, node_list, x_list, y_list, std_list),
                    method=&#39;L-BFGS-B&#39;, jac=True, bounds=bounds,
                    options=options)

                param0 = res.x
                for ii in range(self.nparam):
                    if np.abs(param0[ii]) &gt; 1e-10:
                        bounds[ii] = (param0[ii]-1e-10, param0[ii]+1e-10)
                # print(&#34;bounds&#34;, bounds)

        # Final Training
        lossfunc = opts.get(&#39;lossfunc&#39;,&#39;lstsq&#39;)
        if lossfunc == &#39;lstsq&#39;:
            options = {&#39;maxiter&#39;:niters, &#39;disp&#39;:verbose, &#39;gtol&#39;:1e-10}
            res = sciopt.minimize(
                optimize_obj, param0,
                args=(func, self, nodes, xtrain, ytrain, stdtrain),
                method=&#39;L-BFGS-B&#39;, jac=True,
                options=options)
        elif pyapprox_is_installed is True:
            
            obj = partial(
                optimize_obj,optf=least_squares,graph=self,nodes=nodes,
                xin_l=xtrain, yin_l=ytrain, std_l=stdtrain)
            lamda = opts[&#39;lambda&#39;]
            options = {&#39;ftol&#39;:1e-12,&#39;disp&#39;:False,
                       &#39;maxiter&#39;:1e3, &#39;method&#39;:&#39;slsqp&#39;};
            l1_coef, res = lasso(obj,True,None,param0,lamda,options)
            #res.x includes slack variables so remove these
            res.x=l1_coef
        else:
            raise Exception(&#34;Specified loss is not accepted&#34;)

        self.set_param(res.x)
        return self

def graph_to_vec(graph, attribute=&#39;param&#39;):
    &#34;&#34;&#34;Extract the multifidelity surrogate parameters from the graph

    Parameters
    ----------
    graph : networkx.graph
        The graphical representation of the MF network

    Returns
    -------
        vec : np.ndarray (nparams)
        A flattened array containing all the parameters of the MF network
    &#34;&#34;&#34;
    nodes = graph.nodes
    node_params_dict = nx.get_node_attributes(graph, attribute)
    node_params = np.concatenate([node_params_dict[n] for n in nodes])

    edges = graph.edges
    edge_params_dict = nx.get_edge_attributes(graph, attribute)
    edge_params = np.concatenate([edge_params_dict[e] for e in edges])

    return np.concatenate((node_params, edge_params))

def vec_to_graph(vec, graph, attribute=&#39;param&#39;):
    &#34;&#34;&#34;
    Update the parameters of a multifidelity surrogate

    Parameters
    ----------
    vec : np.ndarray (nparams)
        A flattened array containing all the parameters of the MF network

    graph : networkx.graph
        The graphical representation of the MF network

    Returns
    -------
    graph : networkx.graph
        The updated graphical representation of the MF network with the
        parameter values given by ``vec``.
    &#34;&#34;&#34;
    nodes = graph.nodes
    ind = 0
    for node in nodes:
        try:
            offset = graph.nodes[node][attribute].shape[0]
        except: # What is the exception?
            offset = graph.nodes[node][&#39;param&#39;].shape[0]
        graph.nodes[node][attribute] = vec[ind:ind + offset]
        ind = ind + offset

    edges = graph.edges
    for edge in edges:
        try:
            offset = graph.edges[edge][attribute].shape[0]
        except: # What is the exception?
            offset = graph.edges[edge][&#39;param&#39;].shape[0]

        graph.edges[edge][attribute] = vec[ind:ind + offset]
        ind = ind + offset

    return graph

def optimize_obj(param, optf, graph, nodes, xin_l, yin_l, std_l):
    &#34;&#34;&#34;Composite optimization objective for a set of nodes

    Parameters
    ----------
    param : np.ndarray (nparams)
        The parameter values at which to compute the objective value and
        gradient

    optf : callable
        A scalar valued objective function with the signature

        ``optf(target, predicted) -&gt;  float``

        where ``target`` is a np.ndarray of shape (nobs)
        containing the observations and ``predicted`` is a np.ndarray of
        shape (nobs) containing the model predictions of the observations

    graph : networkx.graph
        The graphical representation of the MF network

    nodes : list
        A list of nodes for which data is available

    xin_l : list
        A list of input features for each node in *nodes*

    yin_l : list
        A list of output values for each node in *nodes*

    std_l : float
        The standard devaition for data for each node in *nodes*

    Returns
    -------
    final_val : float
        The value of the least squares objective function

    final_derivative : np.ndarray (nobs)
        The gradient of ``obj``
    &#34;&#34;&#34;
    final_derivative = np.zeros((param.shape[0]))
    final_val = 0.0

    # print(&#34;nodes =&#34;, nodes)
    for node, xin, yout, std in zip(nodes, xin_l, yin_l, std_l):
        graph.zero_attributes()
        graph.zero_derivatives()
        graph.set_param(param)
        val, anc = graph.forward(xin, node)

        ## optimization function takes
        new_val, obj_grad = optf(yout, val, std=std) 
        derivative = graph.backward(node, obj_grad, ancestors=anc)

        final_val += new_val
        final_derivative += derivative

    return final_val, final_derivative

def learn_obj(param, graph, node, x, y, std):
    &#34;&#34;&#34;
    Return the least squares learning objective function

    Parameters
    ----------
    param : np.ndarray (nparams)
        The parameter values at which to compute the objective value and 
        gradient

    graph : networkx.graph
        The graphical representation of the MF network

    nodes : list 
        A list of nodes for which data is available

    x : list
        A list of input features for each node in *nodes*

    y : list 
        A list of output values for each node in *nodes*

    std : float
        The standard devaition for data for each node in *nodes*

    Returns
    -------
    final_val : float
        The value of the least squares objective function
    &#34;&#34;&#34;
    graph.set_param(param)
    predict, _ = graph.forward(x, node)
    val, _ = least_squares(y, predict, std=std)

    return val

def learn_obj_grad(param, graph, node, x, y, std):
    &#34;&#34;&#34;
    Return the gradient of the least squares learning objective function

    Parameters
    ----------
    param : np.ndarray (nparams)
        The parameter values at which to compute the objective value and
        gradient

    graph : networkx.graph
        The graphical representation of the MF network

    nodes : list
        A list of nodes for which data is available

    x : list
        A list of input features for each node in *nodes*

    y : list
        A list of output values for each node in *nodes*

    std : float
        The standard devaition for data for each node in *nodes*

    Returns
    -------
    final_derivative : np.ndarray (nobs)
        The gradient of the objective
    &#34;&#34;&#34;
    graph.zero_derivatives()
    graph.set_param(param)
    predict, anc = graph.forward(x, node)
    _, grad = least_squares(y, predict, std=std)
    graph.backward(node, grad, ancestors=anc)
    return graph.get_derivative()

def learn_obj_grad_both(param, graph, node, x, y, std):
    &#34;&#34;&#34;
    Return the value and gradient of the least squares learning objective 
    function

    Parameters
    ----------
    param : np.ndarray (nparams)
        The parameter values at which to compute the objective value and 
        gradient

    graph : networkx.graph
        The graphical representation of the MF network

    nodes : list 
        A list of nodes for which data is available

    x : list
        A list of input features for each node in *nodes*

    y : list 
        A list of output values for each node in *nodes*

    std : float
        The standard devaition for data for each node in *nodes*

    Returns
    -------
    val : float
        The value of the least squares objective function

    final_derivative : np.ndarray (nobs)
        The gradient of the objective
    &#34;&#34;&#34;

    graph.zero_derivatives()
    graph.set_param(param)
    predict, A = graph.forward(x, node)
    # print(&#34;predict = &#34;, predict.shape)
    # print(&#34;y = &#34;, y.shape)
    val, grad = least_squares(y, predict, std=std)
    graph.backward(node, grad, ancestors=A)

    return val, graph.get_derivative()

#--------------------------------#
# Functions useful for debugging #

def identity(ynotused, predict, std=None):
    &#34;&#34;&#34; Identity output function &#34;&#34;&#34;
    # f(predict) = predict
    return predict[0], np.ones(predict.shape)

def identity_obj(param, graph, node, x):

    graph.set_param(param)
    predict, _ = graph.forward(x, node)
    return predict[0]

def identity_obj_grad(param, graph, node, x):

    graph.zero_derivatives()
    graph.set_param(param)
    predict, A = graph.forward(x, node)
    # print(&#34;predict = &#34;, predict)
    _, pass_back = identity(None, predict, std=None)
    graph.backward(node, pass_back, ancestors=A)

    return graph.get_derivative()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="net.graph_to_vec"><code class="name flex">
<span>def <span class="ident">graph_to_vec</span></span>(<span>graph, attribute='param')</span>
</code></dt>
<dd>
<div class="desc"><p>Extract the multifidelity surrogate parameters from the graph</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>graph</code></strong> :&ensp;<code>networkx.graph</code></dt>
<dd>The graphical representation of the MF network</dd>
</dl>
<h2 id="returns">Returns</h2>
<pre><code>vec : np.ndarray (nparams)
A flattened array containing all the parameters of the MF network
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def graph_to_vec(graph, attribute=&#39;param&#39;):
    &#34;&#34;&#34;Extract the multifidelity surrogate parameters from the graph

    Parameters
    ----------
    graph : networkx.graph
        The graphical representation of the MF network

    Returns
    -------
        vec : np.ndarray (nparams)
        A flattened array containing all the parameters of the MF network
    &#34;&#34;&#34;
    nodes = graph.nodes
    node_params_dict = nx.get_node_attributes(graph, attribute)
    node_params = np.concatenate([node_params_dict[n] for n in nodes])

    edges = graph.edges
    edge_params_dict = nx.get_edge_attributes(graph, attribute)
    edge_params = np.concatenate([edge_params_dict[e] for e in edges])

    return np.concatenate((node_params, edge_params))</code></pre>
</details>
</dd>
<dt id="net.identity"><code class="name flex">
<span>def <span class="ident">identity</span></span>(<span>ynotused, predict, std=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Identity output function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def identity(ynotused, predict, std=None):
    &#34;&#34;&#34; Identity output function &#34;&#34;&#34;
    # f(predict) = predict
    return predict[0], np.ones(predict.shape)</code></pre>
</details>
</dd>
<dt id="net.identity_obj"><code class="name flex">
<span>def <span class="ident">identity_obj</span></span>(<span>param, graph, node, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def identity_obj(param, graph, node, x):

    graph.set_param(param)
    predict, _ = graph.forward(x, node)
    return predict[0]</code></pre>
</details>
</dd>
<dt id="net.identity_obj_grad"><code class="name flex">
<span>def <span class="ident">identity_obj_grad</span></span>(<span>param, graph, node, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def identity_obj_grad(param, graph, node, x):

    graph.zero_derivatives()
    graph.set_param(param)
    predict, A = graph.forward(x, node)
    # print(&#34;predict = &#34;, predict)
    _, pass_back = identity(None, predict, std=None)
    graph.backward(node, pass_back, ancestors=A)

    return graph.get_derivative()</code></pre>
</details>
</dd>
<dt id="net.learn_obj"><code class="name flex">
<span>def <span class="ident">learn_obj</span></span>(<span>param, graph, node, x, y, std)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the least squares learning objective function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>param</code></strong> :&ensp;<code>np.ndarray (nparams)</code></dt>
<dd>The parameter values at which to compute the objective value and
gradient</dd>
<dt><strong><code>graph</code></strong> :&ensp;<code>networkx.graph</code></dt>
<dd>The graphical representation of the MF network</dd>
<dt><strong><code>nodes</code></strong> :&ensp;<code>list </code></dt>
<dd>A list of nodes for which data is available</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of input features for each node in <em>nodes</em></dd>
<dt><strong><code>y</code></strong> :&ensp;<code>list </code></dt>
<dd>A list of output values for each node in <em>nodes</em></dd>
<dt><strong><code>std</code></strong> :&ensp;<code>float</code></dt>
<dd>The standard devaition for data for each node in <em>nodes</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>final_val</code></strong> :&ensp;<code>float</code></dt>
<dd>The value of the least squares objective function</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn_obj(param, graph, node, x, y, std):
    &#34;&#34;&#34;
    Return the least squares learning objective function

    Parameters
    ----------
    param : np.ndarray (nparams)
        The parameter values at which to compute the objective value and 
        gradient

    graph : networkx.graph
        The graphical representation of the MF network

    nodes : list 
        A list of nodes for which data is available

    x : list
        A list of input features for each node in *nodes*

    y : list 
        A list of output values for each node in *nodes*

    std : float
        The standard devaition for data for each node in *nodes*

    Returns
    -------
    final_val : float
        The value of the least squares objective function
    &#34;&#34;&#34;
    graph.set_param(param)
    predict, _ = graph.forward(x, node)
    val, _ = least_squares(y, predict, std=std)

    return val</code></pre>
</details>
</dd>
<dt id="net.learn_obj_grad"><code class="name flex">
<span>def <span class="ident">learn_obj_grad</span></span>(<span>param, graph, node, x, y, std)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the gradient of the least squares learning objective function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>param</code></strong> :&ensp;<code>np.ndarray (nparams)</code></dt>
<dd>The parameter values at which to compute the objective value and
gradient</dd>
<dt><strong><code>graph</code></strong> :&ensp;<code>networkx.graph</code></dt>
<dd>The graphical representation of the MF network</dd>
<dt><strong><code>nodes</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of nodes for which data is available</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of input features for each node in <em>nodes</em></dd>
<dt><strong><code>y</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of output values for each node in <em>nodes</em></dd>
<dt><strong><code>std</code></strong> :&ensp;<code>float</code></dt>
<dd>The standard devaition for data for each node in <em>nodes</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>final_derivative</code></strong> :&ensp;<code>np.ndarray (nobs)</code></dt>
<dd>The gradient of the objective</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn_obj_grad(param, graph, node, x, y, std):
    &#34;&#34;&#34;
    Return the gradient of the least squares learning objective function

    Parameters
    ----------
    param : np.ndarray (nparams)
        The parameter values at which to compute the objective value and
        gradient

    graph : networkx.graph
        The graphical representation of the MF network

    nodes : list
        A list of nodes for which data is available

    x : list
        A list of input features for each node in *nodes*

    y : list
        A list of output values for each node in *nodes*

    std : float
        The standard devaition for data for each node in *nodes*

    Returns
    -------
    final_derivative : np.ndarray (nobs)
        The gradient of the objective
    &#34;&#34;&#34;
    graph.zero_derivatives()
    graph.set_param(param)
    predict, anc = graph.forward(x, node)
    _, grad = least_squares(y, predict, std=std)
    graph.backward(node, grad, ancestors=anc)
    return graph.get_derivative()</code></pre>
</details>
</dd>
<dt id="net.learn_obj_grad_both"><code class="name flex">
<span>def <span class="ident">learn_obj_grad_both</span></span>(<span>param, graph, node, x, y, std)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the value and gradient of the least squares learning objective
function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>param</code></strong> :&ensp;<code>np.ndarray (nparams)</code></dt>
<dd>The parameter values at which to compute the objective value and
gradient</dd>
<dt><strong><code>graph</code></strong> :&ensp;<code>networkx.graph</code></dt>
<dd>The graphical representation of the MF network</dd>
<dt><strong><code>nodes</code></strong> :&ensp;<code>list </code></dt>
<dd>A list of nodes for which data is available</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of input features for each node in <em>nodes</em></dd>
<dt><strong><code>y</code></strong> :&ensp;<code>list </code></dt>
<dd>A list of output values for each node in <em>nodes</em></dd>
<dt><strong><code>std</code></strong> :&ensp;<code>float</code></dt>
<dd>The standard devaition for data for each node in <em>nodes</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>val</code></strong> :&ensp;<code>float</code></dt>
<dd>The value of the least squares objective function</dd>
<dt><strong><code>final_derivative</code></strong> :&ensp;<code>np.ndarray (nobs)</code></dt>
<dd>The gradient of the objective</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn_obj_grad_both(param, graph, node, x, y, std):
    &#34;&#34;&#34;
    Return the value and gradient of the least squares learning objective 
    function

    Parameters
    ----------
    param : np.ndarray (nparams)
        The parameter values at which to compute the objective value and 
        gradient

    graph : networkx.graph
        The graphical representation of the MF network

    nodes : list 
        A list of nodes for which data is available

    x : list
        A list of input features for each node in *nodes*

    y : list 
        A list of output values for each node in *nodes*

    std : float
        The standard devaition for data for each node in *nodes*

    Returns
    -------
    val : float
        The value of the least squares objective function

    final_derivative : np.ndarray (nobs)
        The gradient of the objective
    &#34;&#34;&#34;

    graph.zero_derivatives()
    graph.set_param(param)
    predict, A = graph.forward(x, node)
    # print(&#34;predict = &#34;, predict.shape)
    # print(&#34;y = &#34;, y.shape)
    val, grad = least_squares(y, predict, std=std)
    graph.backward(node, grad, ancestors=A)

    return val, graph.get_derivative()</code></pre>
</details>
</dd>
<dt id="net.least_squares"><code class="name flex">
<span>def <span class="ident">least_squares</span></span>(<span>target, predicted, std=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the least squares objective function </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>target</code></strong> :&ensp;<code>np.ndarray (nobs)</code></dt>
<dd>The observations</dd>
<dt><strong><code>predicted</code></strong> :&ensp;<code>np.ndarray (nobs)</code></dt>
<dd>The model predictions of the observations</dd>
<dt><strong><code>std</code></strong> :&ensp;<code>float</code></dt>
<dd>The standard deviation of the I.I.D noise</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>obj</code></strong> :&ensp;<code>float</code></dt>
<dd>The value of the least squares objective function</dd>
<dt><strong><code>grad</code></strong> :&ensp;<code>np.ndarray (nobs)</code></dt>
<dd>The gradient of <code>obj</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def least_squares(target, predicted, std=1e0):
    &#34;&#34;&#34; Evaluate the least squares objective function 

    Parameters
    ----------
    target : np.ndarray (nobs)
        The observations

    predicted : np.ndarray (nobs)
        The model predictions of the observations

    std : float
        The standard deviation of the I.I.D noise

    Returns
    -------
    obj : float
        The value of the least squares objective function
    
    grad : np.ndarray (nobs)
        The gradient of ``obj``
    &#34;&#34;&#34;
    resid = target - predicted
    obj = np.dot(resid, resid) * 0.5 * std**-2
    grad = - std**-2 * resid
    return obj, grad</code></pre>
</details>
</dd>
<dt id="net.lin"><code class="name flex">
<span>def <span class="ident">lin</span></span>(<span>param, xinput)</span>
</code></dt>
<dd>
<div class="desc"><p>A linear parametric model </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>param</code></strong> :&ensp;<code>np.ndarray (nparams)</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>The parameters of the model</p>
<dl>
<dt><strong><code>xinput</code></strong> :&ensp;<code>np.ndarray (nsamples,nparams)</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>The independent variables of the model</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>vals</code></strong> :&ensp;<code>np.ndarray (nsamples)</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Evaluation of the linear model</p>
<dl>
<dt><strong><code>grad</code></strong> :&ensp;<code>np.ndarray (nsamples,nparams)</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>gradient of the linear model with respect to the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lin(param, xinput):
    &#34;&#34;&#34;A linear parametric model 

    Parameters
    ----------
    param : np.ndarray (nparams)
       The parameters of the model

    xinput : np.ndarray (nsamples,nparams)
       The independent variables of the model

    Returns
    -------
    vals : np.ndarray (nsamples)
      Evaluation of the linear model

    grad : np.ndarray (nsamples,nparams)
      gradient of the linear model with respect to the model parameters
    &#34;&#34;&#34;
    one = np.ones((xinput.shape[0], 1))
    grad = np.concatenate((one, xinput), axis=1)
    return param[0] + np.dot(param[1:], xinput.T), grad</code></pre>
</details>
</dd>
<dt id="net.monomial_1d_lin"><code class="name flex">
<span>def <span class="ident">monomial_1d_lin</span></span>(<span>param, xinput)</span>
</code></dt>
<dd>
<div class="desc"><p>Linear Model with Monomial basis functions</p>
<p>p[0]+sum(x**p[1:])</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>param</code></strong> :&ensp;<code>np.ndarray (nparams)</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>The parameters of the model</p>
<dl>
<dt><strong><code>xinput</code></strong> :&ensp;<code>np.ndarray (nsamples,nparams)</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>The independent variables of the model</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>vals</code></strong> :&ensp;<code>np.ndarray (nsamples)</code></dt>
<dd>Evaluation of the linear model</dd>
<dt><strong><code>grad</code></strong> :&ensp;<code>np.ndarray (nsamples,nparams)</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>gradient of the linear model with respect to the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def monomial_1d_lin(param, xinput):
    &#34;&#34;&#34;Linear Model with Monomial basis functions

    p[0]+sum(x**p[1:])

    Parameters
    ----------
    param : np.ndarray (nparams)
       The parameters of the model

    xinput : np.ndarray (nsamples,nparams)
       The independent variables of the model

    Returns
    -------
    vals : np.ndarray (nsamples)
        Evaluation of the linear model

    grad : np.ndarray (nsamples,nparams)
      gradient of the linear model with respect to the model parameters
    &#34;&#34;&#34;
    basis = xinput**np.arange(param.shape[0])[np.newaxis, :]
    vals = basis.dot(param)
    grad = basis
    return vals, grad</code></pre>
</details>
</dd>
<dt id="net.optimize_obj"><code class="name flex">
<span>def <span class="ident">optimize_obj</span></span>(<span>param, optf, graph, nodes, xin_l, yin_l, std_l)</span>
</code></dt>
<dd>
<div class="desc"><p>Composite optimization objective for a set of nodes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>param</code></strong> :&ensp;<code>np.ndarray (nparams)</code></dt>
<dd>The parameter values at which to compute the objective value and
gradient</dd>
<dt><strong><code>optf</code></strong> :&ensp;<code>callable</code></dt>
<dd>
<p>A scalar valued objective function with the signature</p>
<p><code>optf(target, predicted) -&gt;
float</code></p>
<p>where <code>target</code> is a np.ndarray of shape (nobs)
containing the observations and <code>predicted</code> is a np.ndarray of
shape (nobs) containing the model predictions of the observations</p>
</dd>
<dt><strong><code>graph</code></strong> :&ensp;<code>networkx.graph</code></dt>
<dd>The graphical representation of the MF network</dd>
<dt><strong><code>nodes</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of nodes for which data is available</dd>
<dt><strong><code>xin_l</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of input features for each node in <em>nodes</em></dd>
<dt><strong><code>yin_l</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of output values for each node in <em>nodes</em></dd>
<dt><strong><code>std_l</code></strong> :&ensp;<code>float</code></dt>
<dd>The standard devaition for data for each node in <em>nodes</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>final_val</code></strong> :&ensp;<code>float</code></dt>
<dd>The value of the least squares objective function</dd>
<dt><strong><code>final_derivative</code></strong> :&ensp;<code>np.ndarray (nobs)</code></dt>
<dd>The gradient of <code>obj</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optimize_obj(param, optf, graph, nodes, xin_l, yin_l, std_l):
    &#34;&#34;&#34;Composite optimization objective for a set of nodes

    Parameters
    ----------
    param : np.ndarray (nparams)
        The parameter values at which to compute the objective value and
        gradient

    optf : callable
        A scalar valued objective function with the signature

        ``optf(target, predicted) -&gt;  float``

        where ``target`` is a np.ndarray of shape (nobs)
        containing the observations and ``predicted`` is a np.ndarray of
        shape (nobs) containing the model predictions of the observations

    graph : networkx.graph
        The graphical representation of the MF network

    nodes : list
        A list of nodes for which data is available

    xin_l : list
        A list of input features for each node in *nodes*

    yin_l : list
        A list of output values for each node in *nodes*

    std_l : float
        The standard devaition for data for each node in *nodes*

    Returns
    -------
    final_val : float
        The value of the least squares objective function

    final_derivative : np.ndarray (nobs)
        The gradient of ``obj``
    &#34;&#34;&#34;
    final_derivative = np.zeros((param.shape[0]))
    final_val = 0.0

    # print(&#34;nodes =&#34;, nodes)
    for node, xin, yout, std in zip(nodes, xin_l, yin_l, std_l):
        graph.zero_attributes()
        graph.zero_derivatives()
        graph.set_param(param)
        val, anc = graph.forward(xin, node)

        ## optimization function takes
        new_val, obj_grad = optf(yout, val, std=std) 
        derivative = graph.backward(node, obj_grad, ancestors=anc)

        final_val += new_val
        final_derivative += derivative

    return final_val, final_derivative</code></pre>
</details>
</dd>
<dt id="net.vec_to_graph"><code class="name flex">
<span>def <span class="ident">vec_to_graph</span></span>(<span>vec, graph, attribute='param')</span>
</code></dt>
<dd>
<div class="desc"><p>Update the parameters of a multifidelity surrogate</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vec</code></strong> :&ensp;<code>np.ndarray (nparams)</code></dt>
<dd>A flattened array containing all the parameters of the MF network</dd>
<dt><strong><code>graph</code></strong> :&ensp;<code>networkx.graph</code></dt>
<dd>The graphical representation of the MF network</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>graph</code></strong> :&ensp;<code>networkx.graph</code></dt>
<dd>The updated graphical representation of the MF network with the
parameter values given by <code>vec</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_to_graph(vec, graph, attribute=&#39;param&#39;):
    &#34;&#34;&#34;
    Update the parameters of a multifidelity surrogate

    Parameters
    ----------
    vec : np.ndarray (nparams)
        A flattened array containing all the parameters of the MF network

    graph : networkx.graph
        The graphical representation of the MF network

    Returns
    -------
    graph : networkx.graph
        The updated graphical representation of the MF network with the
        parameter values given by ``vec``.
    &#34;&#34;&#34;
    nodes = graph.nodes
    ind = 0
    for node in nodes:
        try:
            offset = graph.nodes[node][attribute].shape[0]
        except: # What is the exception?
            offset = graph.nodes[node][&#39;param&#39;].shape[0]
        graph.nodes[node][attribute] = vec[ind:ind + offset]
        ind = ind + offset

    edges = graph.edges
    for edge in edges:
        try:
            offset = graph.edges[edge][attribute].shape[0]
        except: # What is the exception?
            offset = graph.edges[edge][&#39;param&#39;].shape[0]

        graph.edges[edge][attribute] = vec[ind:ind + offset]
        ind = ind + offset

    return graph</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="net.MFSurrogate"><code class="flex name class">
<span>class <span class="ident">MFSurrogate</span></span>
<span>(</span><span>graph, roots, copy_data=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Multifidelity surrogate</p>
<p>A surrogate consists of a graph where the edges and nodes are functions
Each node represents a particular information sources and the edges
describe the relationships between the information sources</p>
<p>Initialize a multifidelity surrogate by providing a graph and the
roots of the graph</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>graph</code></strong> :&ensp;<code>networkx.graph</code></dt>
<dd>The graphical representation of the MF network</dd>
<dt><strong><code>roots</code></strong> :&ensp;<code>list</code></dt>
<dd>The ids of every root node in the network</dd>
<dt><strong><code>copy_data</code></strong> :&ensp;<code>boolean</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>True - perform a deep copy of graph and roots
False - just use a shallow copy (this is dangerous as many functions
change the internal shape)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MFSurrogate():
    &#34;&#34;&#34;Multifidelity surrogate

    A surrogate consists of a graph where the edges and nodes are functions
    Each node represents a particular information sources and the edges
    describe the relationships between the information sources

    &#34;&#34;&#34;
    def __init__(self, graph, roots, copy_data=True):
        &#34;&#34;&#34;Initialize a multifidelity surrogate by providing a graph and the
        roots of the graph

        Parameters
        ----------
        graph : networkx.graph
            The graphical representation of the MF network

        roots : list
            The ids of every root node in the network

        copy_data : boolean
           True - perform a deep copy of graph and roots
           False - just use a shallow copy (this is dangerous as many functions
           change the internal shape)
        &#34;&#34;&#34;
        if copy_data:
            self.roots = copy.deepcopy(roots)
            self.graph = copy.deepcopy(graph)
        else:
            self.roots = roots
            self.graph = graph
            print(&#39;warning MFSurrogate not copying data. proceed with caution&#39;)
        self.nparam = graph_to_vec(self.graph).shape[0]

    def get_nparam(self):
        &#34;&#34;&#34;Number of parameters parameterizing the graph

        Returns
        -------
        nparam : integer
            The number of all the unknown parameters in the MF surrogate&#34;&#34;&#34;
        return self.nparam

    def forward(self, xinput, target_node):
        &#34;&#34;&#34;Evaluate the surrogate output at target_node by considering the
        subgraph of all ancestors of this node

        Parameters
        ----------
        xinput : np.ndarray (nsamples,nparams)
            The independent variables of the model

        target_node : integer
            The id of the node under consideration

        Returns
        -------
        This function adds the following attributes to the underlying graph

        eval :
            stores the evaluation of the function represented by the
            particular node / edge the evaluations at the nodes are
            cumulative (summing up all ancestors) whereas the edges are local

        pre-grad : np.ndarray
            internal attribute needed for accounting

        parents-left : set
            internal attribute needed for accounting

        &#34;&#34;&#34;
        anc = nx.ancestors(self.graph, target_node)
        anc_and_target = anc.union(set([target_node]))
        relevant_root_nodes = anc.intersection(self.roots)

        # Evaluate the target nodes and all ancestral nodes and put the root
        # nodes in a queue
        queue = SimpleQueue()
        for node in anc_and_target:
            pval, pgrad = self.graph.nodes[node][&#39;func&#39;](
                self.graph.nodes[node][&#39;param&#39;], xinput)
            self.graph.nodes[node][&#39;eval&#39;] = pval
            self.graph.nodes[node][&#39;pre_grad&#39;] = pgrad
            self.graph.nodes[node][&#39;parents_left&#39;] = set(
                self.graph.predecessors(node))

            if node in relevant_root_nodes:
                queue.put(node)

        while not queue.empty():

            node = queue.get()
            feval = self.graph.nodes[node][&#39;eval&#39;]
            for child in self.graph.successors(node):
                if child in anc_and_target:
                    pval, pgrad = self.graph.edges[node, child][&#39;func&#39;](
                        self.graph.edges[node, child][&#39;param&#39;], xinput)

                    self.graph.nodes[child][&#39;eval&#39;] += feval * pval

                    ftile = np.tile(feval.reshape(
                        (feval.shape[0], 1)), (1, pgrad.shape[1]))
                    self.graph.edges[node, child][&#39;pre_grad&#39;] = ftile * pgrad
                    self.graph.edges[node, child][&#39;eval&#39;] = pval

                    self.graph.nodes[child][&#39;parents_left&#39;].remove(node)

                    if self.graph.nodes[child][&#39;parents_left&#39;] == set():
                        queue.put(child)

        return self.graph.nodes[node][&#39;eval&#39;], anc

    def backward(self, target_node, deriv_pass, ancestors=None):
        &#34;&#34;&#34;Perform a backward computation to compute the derivatives for all
        parameters that affect the target node

        Parameters
        ----------
        target_node : integer
            The id of the node under consideration

        deriv_pass : np.ndarray (nparams)
            A gradient vector

        Returns
        -------
        derivative : np.ndarray(nparams)
            A vector containing the derivative of all parameters
        &#34;&#34;&#34;

        if ancestors is None:
            ancestors = nx.ancestors(self.graph, target_node)

        anc_and_target = ancestors.union(set([target_node]))


        # Evaluate the node
        self.graph.nodes[target_node][&#39;pass_down&#39;] = deriv_pass

        # Gradient with respect to beta
        self.graph.nodes[target_node][&#39;derivative&#39;] = \
            np.dot(self.graph.nodes[target_node][&#39;pass_down&#39;],
                   self.graph.nodes[target_node][&#39;pre_grad&#39;])

        queue = SimpleQueue()
        queue.put(target_node)

        for node in ancestors:
            self.graph.nodes[node][&#39;children_left&#39;] = set(
                self.graph.successors(node)).intersection(anc_and_target)
            self.graph.nodes[node][&#39;pass_down&#39;] = 0.0
            self.graph.nodes[node][&#39;derivative&#39;] = 0.0


        while not queue.empty():
            node = queue.get()

            pass_down = self.graph.nodes[node][&#39;pass_down&#39;]
            for parent in self.graph.predecessors(node):
                self.graph.nodes[parent][&#39;pass_down&#39;] += \
                    pass_down * self.graph.edges[parent, node][&#39;eval&#39;]
                self.graph.edges[parent, node][&#39;derivative&#39;] = \
                    np.dot(pass_down,self.graph.edges[parent, node][&#39;pre_grad&#39;])
                self.graph.nodes[parent][&#39;derivative&#39;] += \
                    np.dot(pass_down * self.graph.edges[parent, node][&#39;eval&#39;],
                           self.graph.nodes[parent][&#39;pre_grad&#39;])

                self.graph.nodes[parent][&#39;children_left&#39;].remove(node)
                if self.graph.nodes[parent][&#39;children_left&#39;] == set():
                    queue.put(parent)

        return self.get_derivative()

    def set_param(self, param):
        &#34;&#34;&#34;Set the parameters for the graph

        Parameters
        ----------
        param : np.ndarray (nparams)
            A flattened array containing all parameters of the MF surrogate
        &#34;&#34;&#34;
        self.graph = vec_to_graph(param, self.graph, attribute=&#39;param&#39;)

    def get_param(self):
        &#34;&#34;&#34;Get the parameters of the graph &#34;&#34;&#34;
        return graph_to_vec(self.graph, attribute=&#39;param&#39;)

    def get_derivative(self):
        &#34;&#34;&#34;Get a vector of derivatives of each parameter &#34;&#34;&#34;
        return graph_to_vec(self.graph, attribute=&#39;derivative&#39;)

    def get_evals(self):
        &#34;&#34;&#34;Get the evaluations at each node &#34;&#34;&#34;
        return [self.graph.nodes[node][&#39;eval&#39;] for node in self.graph.nodes]

    def zero_derivatives(self):
        &#34;&#34;&#34;Set all the derivative attributes to zero

        Used prior to computing a new derivative to clear out previous sweep
        &#34;&#34;&#34;
        self.graph = vec_to_graph(
            np.zeros(self.nparam), self.graph, attribute=&#39;derivative&#39;)

    def zero_attributes(self):
        &#34;&#34;&#34;Zero all attributes except &#39;func&#39; and &#39;param&#39; &#34;&#34;&#34;

        atts = [&#39;eval&#39;, &#39;pass_down&#39;, &#39;pre_grad&#39;, &#39;derivative&#39;,
                &#39;children_left&#39;, &#39;parents_left&#39;]
        for att in atts:
            for node in self.graph.nodes:
                try:
                    self.graph.nodes[node][att] = 0.0
                except: # what exception is it?
                    continue
            for edge in self.graph.edges:
                try:
                    self.graph.edges[edge][att] = 0.0
                except: # what exception is it?
                    continue


    def train(self, param0in, nodes, xtrain, ytrain, stdtrain, niters=200,
              func=least_squares,
              verbose=False, warmup=True, opts=dict()):
        &#34;&#34;&#34;Train the multifidelity surrogate.

        This is the main entrance point for data-driven training.

        Parameters
        ----------
        param0in : np.ndarray (nparams)
            The initial guess for the parameters

        nodes : list
            A list of nodes for which data is available

        xtrain : list
            A list of input features for each node in *nodes*

        ytrain : list
            A list of output values for each node in *nodes*

        stdtrain : float
            The standard devaition for data for each node in *nodes*

        niters : integer
            The number of optimization iterations

        func : callable
            A scalar valued objective function with the signature

            ``func(target, predicted) -&gt;  val (float), grad (np.ndarray)``

            where ``target`` is a np.ndarray of shape (nobs)
            containing the observations and ``predicted`` is a np.ndarray of
            shape (nobs) containing the model predictions of the observations

        verbose : integer
            The verbosity level

        warmup : boolean
            Specify whether or not to progressively find a good guess before
            optimizing

        opts : dictionary
            Specify the type of loss function: &#39;lstsq&#39; for squared error, anything else for L1 regularization, &#39;lambda&#39; for regularization value
        
        Returns
        -------
        Upon completion of this function, the parameters of the graph are set
        to the values that best fit the data, as defined by *func*
        &#34;&#34;&#34;
        bounds = list(zip([-np.inf]*self.nparam, [np.inf]*self.nparam))
        param0 = copy.deepcopy(param0in)

        # options = {&#39;maxiter&#39;:20, &#39;disp&#39;:False, &#39;gtol&#39;:1e-10, &#39;ftol&#39;:1e-18}
        options = {&#39;maxiter&#39;:20, &#39;disp&#39;:False, &#39;gtol&#39;:1e-10, &#39;ftol&#39;:1e-18}

        # Warming up
        if warmup is True:
            for node in nodes:

                node_list = nodes[node-1:node]
                x_list = xtrain[node-1:node]
                y_list = ytrain[node-1:node]
                std_list = stdtrain[node-1:node]

                res = sciopt.minimize(
                    optimize_obj, param0,
                    args=(func, self, node_list, x_list, y_list, std_list),
                    method=&#39;L-BFGS-B&#39;, jac=True, bounds=bounds,
                    options=options)

                param0 = res.x
                for ii in range(self.nparam):
                    if np.abs(param0[ii]) &gt; 1e-10:
                        bounds[ii] = (param0[ii]-1e-10, param0[ii]+1e-10)
                # print(&#34;bounds&#34;, bounds)

        # Final Training
        lossfunc = opts.get(&#39;lossfunc&#39;,&#39;lstsq&#39;)
        if lossfunc == &#39;lstsq&#39;:
            options = {&#39;maxiter&#39;:niters, &#39;disp&#39;:verbose, &#39;gtol&#39;:1e-10}
            res = sciopt.minimize(
                optimize_obj, param0,
                args=(func, self, nodes, xtrain, ytrain, stdtrain),
                method=&#39;L-BFGS-B&#39;, jac=True,
                options=options)
        elif pyapprox_is_installed is True:
            
            obj = partial(
                optimize_obj,optf=least_squares,graph=self,nodes=nodes,
                xin_l=xtrain, yin_l=ytrain, std_l=stdtrain)
            lamda = opts[&#39;lambda&#39;]
            options = {&#39;ftol&#39;:1e-12,&#39;disp&#39;:False,
                       &#39;maxiter&#39;:1e3, &#39;method&#39;:&#39;slsqp&#39;};
            l1_coef, res = lasso(obj,True,None,param0,lamda,options)
            #res.x includes slack variables so remove these
            res.x=l1_coef
        else:
            raise Exception(&#34;Specified loss is not accepted&#34;)

        self.set_param(res.x)
        return self</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="net.MFSurrogate.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>self, target_node, deriv_pass, ancestors=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a backward computation to compute the derivatives for all
parameters that affect the target node</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>target_node</code></strong> :&ensp;<code>integer</code></dt>
<dd>The id of the node under consideration</dd>
<dt><strong><code>deriv_pass</code></strong> :&ensp;<code>np.ndarray (nparams)</code></dt>
<dd>A gradient vector</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>derivative</code></strong> :&ensp;<code>np.ndarray(nparams)</code></dt>
<dd>A vector containing the derivative of all parameters</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backward(self, target_node, deriv_pass, ancestors=None):
    &#34;&#34;&#34;Perform a backward computation to compute the derivatives for all
    parameters that affect the target node

    Parameters
    ----------
    target_node : integer
        The id of the node under consideration

    deriv_pass : np.ndarray (nparams)
        A gradient vector

    Returns
    -------
    derivative : np.ndarray(nparams)
        A vector containing the derivative of all parameters
    &#34;&#34;&#34;

    if ancestors is None:
        ancestors = nx.ancestors(self.graph, target_node)

    anc_and_target = ancestors.union(set([target_node]))


    # Evaluate the node
    self.graph.nodes[target_node][&#39;pass_down&#39;] = deriv_pass

    # Gradient with respect to beta
    self.graph.nodes[target_node][&#39;derivative&#39;] = \
        np.dot(self.graph.nodes[target_node][&#39;pass_down&#39;],
               self.graph.nodes[target_node][&#39;pre_grad&#39;])

    queue = SimpleQueue()
    queue.put(target_node)

    for node in ancestors:
        self.graph.nodes[node][&#39;children_left&#39;] = set(
            self.graph.successors(node)).intersection(anc_and_target)
        self.graph.nodes[node][&#39;pass_down&#39;] = 0.0
        self.graph.nodes[node][&#39;derivative&#39;] = 0.0


    while not queue.empty():
        node = queue.get()

        pass_down = self.graph.nodes[node][&#39;pass_down&#39;]
        for parent in self.graph.predecessors(node):
            self.graph.nodes[parent][&#39;pass_down&#39;] += \
                pass_down * self.graph.edges[parent, node][&#39;eval&#39;]
            self.graph.edges[parent, node][&#39;derivative&#39;] = \
                np.dot(pass_down,self.graph.edges[parent, node][&#39;pre_grad&#39;])
            self.graph.nodes[parent][&#39;derivative&#39;] += \
                np.dot(pass_down * self.graph.edges[parent, node][&#39;eval&#39;],
                       self.graph.nodes[parent][&#39;pre_grad&#39;])

            self.graph.nodes[parent][&#39;children_left&#39;].remove(node)
            if self.graph.nodes[parent][&#39;children_left&#39;] == set():
                queue.put(parent)

    return self.get_derivative()</code></pre>
</details>
</dd>
<dt id="net.MFSurrogate.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, xinput, target_node)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the surrogate output at target_node by considering the
subgraph of all ancestors of this node</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>xinput</code></strong> :&ensp;<code>np.ndarray (nsamples,nparams)</code></dt>
<dd>The independent variables of the model</dd>
<dt><strong><code>target_node</code></strong> :&ensp;<code>integer</code></dt>
<dd>The id of the node under consideration</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>This function adds the following attributes to the underlying graph</code></dt>
<dd>&nbsp;</dd>
<dt><code>eval :</code></dt>
<dd>stores the evaluation of the function represented by the
particular node / edge the evaluations at the nodes are
cumulative (summing up all ancestors) whereas the edges are local</dd>
<dt><code>pre-grad : np.ndarray</code></dt>
<dd>internal attribute needed for accounting</dd>
<dt><code>parents-left : set</code></dt>
<dd>internal attribute needed for accounting</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, xinput, target_node):
    &#34;&#34;&#34;Evaluate the surrogate output at target_node by considering the
    subgraph of all ancestors of this node

    Parameters
    ----------
    xinput : np.ndarray (nsamples,nparams)
        The independent variables of the model

    target_node : integer
        The id of the node under consideration

    Returns
    -------
    This function adds the following attributes to the underlying graph

    eval :
        stores the evaluation of the function represented by the
        particular node / edge the evaluations at the nodes are
        cumulative (summing up all ancestors) whereas the edges are local

    pre-grad : np.ndarray
        internal attribute needed for accounting

    parents-left : set
        internal attribute needed for accounting

    &#34;&#34;&#34;
    anc = nx.ancestors(self.graph, target_node)
    anc_and_target = anc.union(set([target_node]))
    relevant_root_nodes = anc.intersection(self.roots)

    # Evaluate the target nodes and all ancestral nodes and put the root
    # nodes in a queue
    queue = SimpleQueue()
    for node in anc_and_target:
        pval, pgrad = self.graph.nodes[node][&#39;func&#39;](
            self.graph.nodes[node][&#39;param&#39;], xinput)
        self.graph.nodes[node][&#39;eval&#39;] = pval
        self.graph.nodes[node][&#39;pre_grad&#39;] = pgrad
        self.graph.nodes[node][&#39;parents_left&#39;] = set(
            self.graph.predecessors(node))

        if node in relevant_root_nodes:
            queue.put(node)

    while not queue.empty():

        node = queue.get()
        feval = self.graph.nodes[node][&#39;eval&#39;]
        for child in self.graph.successors(node):
            if child in anc_and_target:
                pval, pgrad = self.graph.edges[node, child][&#39;func&#39;](
                    self.graph.edges[node, child][&#39;param&#39;], xinput)

                self.graph.nodes[child][&#39;eval&#39;] += feval * pval

                ftile = np.tile(feval.reshape(
                    (feval.shape[0], 1)), (1, pgrad.shape[1]))
                self.graph.edges[node, child][&#39;pre_grad&#39;] = ftile * pgrad
                self.graph.edges[node, child][&#39;eval&#39;] = pval

                self.graph.nodes[child][&#39;parents_left&#39;].remove(node)

                if self.graph.nodes[child][&#39;parents_left&#39;] == set():
                    queue.put(child)

    return self.graph.nodes[node][&#39;eval&#39;], anc</code></pre>
</details>
</dd>
<dt id="net.MFSurrogate.get_derivative"><code class="name flex">
<span>def <span class="ident">get_derivative</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get a vector of derivatives of each parameter</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_derivative(self):
    &#34;&#34;&#34;Get a vector of derivatives of each parameter &#34;&#34;&#34;
    return graph_to_vec(self.graph, attribute=&#39;derivative&#39;)</code></pre>
</details>
</dd>
<dt id="net.MFSurrogate.get_evals"><code class="name flex">
<span>def <span class="ident">get_evals</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the evaluations at each node</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_evals(self):
    &#34;&#34;&#34;Get the evaluations at each node &#34;&#34;&#34;
    return [self.graph.nodes[node][&#39;eval&#39;] for node in self.graph.nodes]</code></pre>
</details>
</dd>
<dt id="net.MFSurrogate.get_nparam"><code class="name flex">
<span>def <span class="ident">get_nparam</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Number of parameters parameterizing the graph</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>nparam</code></strong> :&ensp;<code>integer</code></dt>
<dd>The number of all the unknown parameters in the MF surrogate</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_nparam(self):
    &#34;&#34;&#34;Number of parameters parameterizing the graph

    Returns
    -------
    nparam : integer
        The number of all the unknown parameters in the MF surrogate&#34;&#34;&#34;
    return self.nparam</code></pre>
</details>
</dd>
<dt id="net.MFSurrogate.get_param"><code class="name flex">
<span>def <span class="ident">get_param</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the parameters of the graph</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_param(self):
    &#34;&#34;&#34;Get the parameters of the graph &#34;&#34;&#34;
    return graph_to_vec(self.graph, attribute=&#39;param&#39;)</code></pre>
</details>
</dd>
<dt id="net.MFSurrogate.set_param"><code class="name flex">
<span>def <span class="ident">set_param</span></span>(<span>self, param)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the parameters for the graph</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>param</code></strong> :&ensp;<code>np.ndarray (nparams)</code></dt>
<dd>A flattened array containing all parameters of the MF surrogate</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_param(self, param):
    &#34;&#34;&#34;Set the parameters for the graph

    Parameters
    ----------
    param : np.ndarray (nparams)
        A flattened array containing all parameters of the MF surrogate
    &#34;&#34;&#34;
    self.graph = vec_to_graph(param, self.graph, attribute=&#39;param&#39;)</code></pre>
</details>
</dd>
<dt id="net.MFSurrogate.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, param0in, nodes, xtrain, ytrain, stdtrain, niters=200, func=&lt;function least_squares&gt;, verbose=False, warmup=True, opts={})</span>
</code></dt>
<dd>
<div class="desc"><p>Train the multifidelity surrogate.</p>
<p>This is the main entrance point for data-driven training.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>param0in</code></strong> :&ensp;<code>np.ndarray (nparams)</code></dt>
<dd>The initial guess for the parameters</dd>
<dt><strong><code>nodes</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of nodes for which data is available</dd>
<dt><strong><code>xtrain</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of input features for each node in <em>nodes</em></dd>
<dt><strong><code>ytrain</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of output values for each node in <em>nodes</em></dd>
<dt><strong><code>stdtrain</code></strong> :&ensp;<code>float</code></dt>
<dd>The standard devaition for data for each node in <em>nodes</em></dd>
<dt><strong><code>niters</code></strong> :&ensp;<code>integer</code></dt>
<dd>The number of optimization iterations</dd>
<dt><strong><code>func</code></strong> :&ensp;<code>callable</code></dt>
<dd>
<p>A scalar valued objective function with the signature</p>
<p><code>func(target, predicted) -&gt;
val (float), grad (np.ndarray)</code></p>
<p>where <code>target</code> is a np.ndarray of shape (nobs)
containing the observations and <code>predicted</code> is a np.ndarray of
shape (nobs) containing the model predictions of the observations</p>
</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>integer</code></dt>
<dd>The verbosity level</dd>
<dt><strong><code>warmup</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Specify whether or not to progressively find a good guess before
optimizing</dd>
<dt><strong><code>opts</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Specify the type of loss function: 'lstsq' for squared error, anything else for L1 regularization, 'lambda' for regularization value</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Upon completion</code> of <code>this function, the parameters</code> of <code>the graph are set</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>to the values that best fit the data, as defined by <em>func</em></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, param0in, nodes, xtrain, ytrain, stdtrain, niters=200,
          func=least_squares,
          verbose=False, warmup=True, opts=dict()):
    &#34;&#34;&#34;Train the multifidelity surrogate.

    This is the main entrance point for data-driven training.

    Parameters
    ----------
    param0in : np.ndarray (nparams)
        The initial guess for the parameters

    nodes : list
        A list of nodes for which data is available

    xtrain : list
        A list of input features for each node in *nodes*

    ytrain : list
        A list of output values for each node in *nodes*

    stdtrain : float
        The standard devaition for data for each node in *nodes*

    niters : integer
        The number of optimization iterations

    func : callable
        A scalar valued objective function with the signature

        ``func(target, predicted) -&gt;  val (float), grad (np.ndarray)``

        where ``target`` is a np.ndarray of shape (nobs)
        containing the observations and ``predicted`` is a np.ndarray of
        shape (nobs) containing the model predictions of the observations

    verbose : integer
        The verbosity level

    warmup : boolean
        Specify whether or not to progressively find a good guess before
        optimizing

    opts : dictionary
        Specify the type of loss function: &#39;lstsq&#39; for squared error, anything else for L1 regularization, &#39;lambda&#39; for regularization value
    
    Returns
    -------
    Upon completion of this function, the parameters of the graph are set
    to the values that best fit the data, as defined by *func*
    &#34;&#34;&#34;
    bounds = list(zip([-np.inf]*self.nparam, [np.inf]*self.nparam))
    param0 = copy.deepcopy(param0in)

    # options = {&#39;maxiter&#39;:20, &#39;disp&#39;:False, &#39;gtol&#39;:1e-10, &#39;ftol&#39;:1e-18}
    options = {&#39;maxiter&#39;:20, &#39;disp&#39;:False, &#39;gtol&#39;:1e-10, &#39;ftol&#39;:1e-18}

    # Warming up
    if warmup is True:
        for node in nodes:

            node_list = nodes[node-1:node]
            x_list = xtrain[node-1:node]
            y_list = ytrain[node-1:node]
            std_list = stdtrain[node-1:node]

            res = sciopt.minimize(
                optimize_obj, param0,
                args=(func, self, node_list, x_list, y_list, std_list),
                method=&#39;L-BFGS-B&#39;, jac=True, bounds=bounds,
                options=options)

            param0 = res.x
            for ii in range(self.nparam):
                if np.abs(param0[ii]) &gt; 1e-10:
                    bounds[ii] = (param0[ii]-1e-10, param0[ii]+1e-10)
            # print(&#34;bounds&#34;, bounds)

    # Final Training
    lossfunc = opts.get(&#39;lossfunc&#39;,&#39;lstsq&#39;)
    if lossfunc == &#39;lstsq&#39;:
        options = {&#39;maxiter&#39;:niters, &#39;disp&#39;:verbose, &#39;gtol&#39;:1e-10}
        res = sciopt.minimize(
            optimize_obj, param0,
            args=(func, self, nodes, xtrain, ytrain, stdtrain),
            method=&#39;L-BFGS-B&#39;, jac=True,
            options=options)
    elif pyapprox_is_installed is True:
        
        obj = partial(
            optimize_obj,optf=least_squares,graph=self,nodes=nodes,
            xin_l=xtrain, yin_l=ytrain, std_l=stdtrain)
        lamda = opts[&#39;lambda&#39;]
        options = {&#39;ftol&#39;:1e-12,&#39;disp&#39;:False,
                   &#39;maxiter&#39;:1e3, &#39;method&#39;:&#39;slsqp&#39;};
        l1_coef, res = lasso(obj,True,None,param0,lamda,options)
        #res.x includes slack variables so remove these
        res.x=l1_coef
    else:
        raise Exception(&#34;Specified loss is not accepted&#34;)

    self.set_param(res.x)
    return self</code></pre>
</details>
</dd>
<dt id="net.MFSurrogate.zero_attributes"><code class="name flex">
<span>def <span class="ident">zero_attributes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Zero all attributes except 'func' and 'param'</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zero_attributes(self):
    &#34;&#34;&#34;Zero all attributes except &#39;func&#39; and &#39;param&#39; &#34;&#34;&#34;

    atts = [&#39;eval&#39;, &#39;pass_down&#39;, &#39;pre_grad&#39;, &#39;derivative&#39;,
            &#39;children_left&#39;, &#39;parents_left&#39;]
    for att in atts:
        for node in self.graph.nodes:
            try:
                self.graph.nodes[node][att] = 0.0
            except: # what exception is it?
                continue
        for edge in self.graph.edges:
            try:
                self.graph.edges[edge][att] = 0.0
            except: # what exception is it?
                continue</code></pre>
</details>
</dd>
<dt id="net.MFSurrogate.zero_derivatives"><code class="name flex">
<span>def <span class="ident">zero_derivatives</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Set all the derivative attributes to zero</p>
<p>Used prior to computing a new derivative to clear out previous sweep</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zero_derivatives(self):
    &#34;&#34;&#34;Set all the derivative attributes to zero

    Used prior to computing a new derivative to clear out previous sweep
    &#34;&#34;&#34;
    self.graph = vec_to_graph(
        np.zeros(self.nparam), self.graph, attribute=&#39;derivative&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="net.graph_to_vec" href="#net.graph_to_vec">graph_to_vec</a></code></li>
<li><code><a title="net.identity" href="#net.identity">identity</a></code></li>
<li><code><a title="net.identity_obj" href="#net.identity_obj">identity_obj</a></code></li>
<li><code><a title="net.identity_obj_grad" href="#net.identity_obj_grad">identity_obj_grad</a></code></li>
<li><code><a title="net.learn_obj" href="#net.learn_obj">learn_obj</a></code></li>
<li><code><a title="net.learn_obj_grad" href="#net.learn_obj_grad">learn_obj_grad</a></code></li>
<li><code><a title="net.learn_obj_grad_both" href="#net.learn_obj_grad_both">learn_obj_grad_both</a></code></li>
<li><code><a title="net.least_squares" href="#net.least_squares">least_squares</a></code></li>
<li><code><a title="net.lin" href="#net.lin">lin</a></code></li>
<li><code><a title="net.monomial_1d_lin" href="#net.monomial_1d_lin">monomial_1d_lin</a></code></li>
<li><code><a title="net.optimize_obj" href="#net.optimize_obj">optimize_obj</a></code></li>
<li><code><a title="net.vec_to_graph" href="#net.vec_to_graph">vec_to_graph</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="net.MFSurrogate" href="#net.MFSurrogate">MFSurrogate</a></code></h4>
<ul class="two-column">
<li><code><a title="net.MFSurrogate.backward" href="#net.MFSurrogate.backward">backward</a></code></li>
<li><code><a title="net.MFSurrogate.forward" href="#net.MFSurrogate.forward">forward</a></code></li>
<li><code><a title="net.MFSurrogate.get_derivative" href="#net.MFSurrogate.get_derivative">get_derivative</a></code></li>
<li><code><a title="net.MFSurrogate.get_evals" href="#net.MFSurrogate.get_evals">get_evals</a></code></li>
<li><code><a title="net.MFSurrogate.get_nparam" href="#net.MFSurrogate.get_nparam">get_nparam</a></code></li>
<li><code><a title="net.MFSurrogate.get_param" href="#net.MFSurrogate.get_param">get_param</a></code></li>
<li><code><a title="net.MFSurrogate.set_param" href="#net.MFSurrogate.set_param">set_param</a></code></li>
<li><code><a title="net.MFSurrogate.train" href="#net.MFSurrogate.train">train</a></code></li>
<li><code><a title="net.MFSurrogate.zero_attributes" href="#net.MFSurrogate.zero_attributes">zero_attributes</a></code></li>
<li><code><a title="net.MFSurrogate.zero_derivatives" href="#net.MFSurrogate.zero_derivatives">zero_derivatives</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>